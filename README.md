#  LLM Generated Code Evaluation

This repository provides a **framework to evaluate code generated by Large Language Models (LLMs)**.  
The goal is to assess correctness, functionality, compilation success, and code quality of automatically generated code.

---

##  Features

- ✅ **Compilation Check** – Verifies if generated code compiles without errors.  
- ✅ **Correctness Evaluation** – Compares LLM output against expected behavior/test cases.  
- ✅ **Functionality Testing** – Runs unit/integration tests to confirm code works as intended.  
- ✅ **Code Quality Analysis** – Uses tools like **Clang-Tidy**, **Cppcheck**, and linters.  
- ✅ **Automation Ready** – Scripts to streamline evaluation at scale.  ( in progress )

---

##  Installation

The required dependencies are taken care of in Docker file

